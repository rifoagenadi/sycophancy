{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save batch API result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"\")\n",
    " \n",
    "batch_list = client.batches.list(limit=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing  mmlu-gemma-3_intervened_64_-20.0\n",
      "writing  mmlu-gemma-3_base\n",
      "writing  truthfulqa-gemma-3_base\n",
      "writing  truthfulqa-gemma-3_final_iti_8_-20.0_mha_layer_14\n",
      "writing  truthfulqa-gemma-3_initial_iti_8_-20.0_mha_layer_14\n",
      "writing  truthfulqa-gemma-3_final_iti_8_-20.0_mha_layer_12\n",
      "writing  truthfulqa-gemma-3_initial_iti_8_-20.0_mha_layer_12\n",
      "writing  truthfulqa-gemma-3_final_iti_8_-20.0\n",
      "writing  truthfulqa-gemma-3_final_iti_1_20.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_1_20.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_1_10.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_1_10.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_1_-10.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_1_-10.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_20.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_20.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_10.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_10.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-10.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-10.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_4_-10.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_4_-10.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_4_-20.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_4_-20.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_2_20.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_2_20.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_2_10.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_2_10.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_2_-10.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_2_-10.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_2_-20.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_2_-20.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_64_-20.0\n",
      "writing  truthfulqa-gemma-3_initial_iti_64_-20.0\n",
      "writing  truthfulqa-gemma-3_final_iti_64_-20.0\n",
      "writing  truthfulqa-gemma-3_initial_iti_64_-20.0\n",
      "writing  truthfulqa-llama_final_iti_1_-10.0_residual\n",
      "writing  truthfulqa-llama_initial_iti_1_-10.0_residual\n",
      "writing  truthfulqa-llama_final_iti_1_-5.0_residual\n",
      "writing  truthfulqa-llama_initial_iti_1_-5.0_residual\n",
      "writing  truthfulqa-llama_final_iti_1_-2.0_residual\n",
      "writing  truthfulqa-llama_initial_iti_1_-2.0_residual\n",
      "writing  truthfulqa-llama_final_iti_8_-20.0_residual\n",
      "writing  truthfulqa-llama_initial_iti_8_-20.0_residual\n",
      "writing  truthfulqa-gemma-3_final_iti_1_0.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_1_0.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_1_-0.5_mlp\n",
      "writing  truthfulqa-gemma-3_initial_iti_1_-0.5_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-0.5_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-0.5_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_0.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_0.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_4_-5.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_4_-5.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-10.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-10.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_5.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_5.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_2.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_2.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-2.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-2.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-5.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-llama_final_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-llama_initial_iti_1_-20.0_mlp\n",
      "writing  truthfulqa-gemma-3_final_iti_64_-20.0_random\n",
      "writing  truthfulqa-gemma-3_initial_iti_64_-20.0_random\n",
      "writing  truthfulqa-llama_final_iti_16_-20.0_random\n",
      "writing  truthfulqa-llama_initial_iti_16_-20.0_random\n",
      "writing  truthfulqa-gemma-3_final_iti_64_0.0\n",
      "writing  truthfulqa-gemma-3_initial_iti_64_0.0\n",
      "writing  truthfulqa-llama_final_iti_16_0.0\n",
      "writing  truthfulqa-llama_initial_iti_16_0.0\n",
      "writing  truthfulqa-llama_final_iti_16_0.0\n",
      "writing  truthfulqa-llama_initial_iti_16_0.0\n",
      "writing  truthfulqa-llama_final_iti_16_-20.0\n",
      "writing  truthfulqa-llama_initial_iti_16_-20.0\n",
      "writing  truthfulqa-gemma-3_final_iti_64_0.0\n",
      "writing  truthfulqa-gemma-3_initial_iti_64_0.0\n",
      "writing  truthfulqa-llama_final_iti_16_0.0\n",
      "writing  truthfulqa-llama_initial_iti_16_0.0\n",
      "writing  truthfulqa-gemma-3_final_prompt\n",
      "writing  truthfulqa-gemma-3_initial_prompt\n",
      "writing  truthfulqa-llama_final_prompt\n",
      "writing  truthfulqa-llama_initial_prompt\n",
      "writing  truthfulqa-llama_final_base\n",
      "writing  truthfulqa-llama_initial_base\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for current_batch in batch_list:\n",
    "    output_filename = current_batch.metadata['description']\n",
    "    output_file_id = current_batch.output_file_id\n",
    "    current_file = client.files.content(output_file_id).content\n",
    "    print('writing ', output_filename)\n",
    "    with open(f\"evaluation_jsonl/{output_filename}.jsonl\", 'wb') as file:\n",
    "        file.write(current_file)\n",
    "    i+=1\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.5182926829268293], [0.49390243902439024], [0.41463414634146345]],\n",
       " [[0.41463414634146345], [0.38414634146341464], [0.3597560975609756]],\n",
       " [[0.43529411764705883], [0.4567901234567901], [0.4411764705882353]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_accuracy(predictions: list[str]):\n",
    "    incorrect_count = 0\n",
    "    for item in predictions:\n",
    "        if \"INCORRECT\" in item:\n",
    "            incorrect_count += 1\n",
    "    return 1-incorrect_count/len(predictions)\n",
    "\n",
    "import json\n",
    "model='gemma-3'\n",
    "TOP_K_HEADS=[1, 2, 4]\n",
    "SCALES=[-10]\n",
    "\n",
    "initial_accuracies = [] # list of list [[for strength in scales] for k in k_heads]\n",
    "final_accuracies = []\n",
    "shifts = []\n",
    "kl_divergences = [[0.03, 0.01, 0.01, 0.03], [0.07, 0.02, 0.03, 0.12], [0.12, 0.03, 0.04, 0.18], [0.15, 0.04, 0.05, 0.21]]\n",
    "for k in TOP_K_HEADS:\n",
    "    current_k_initial_accuracies = []\n",
    "    current_k_final_accuracies = []\n",
    "    current_k_shifts = []\n",
    "    for strength in SCALES:\n",
    "        initial_predictions = []\n",
    "        # with open(f'evaluation_jsonl/mmlu-{model}_intervened_64_-20.0.jsonl', 'r') as file:\n",
    "        # with open(f'evaluation_jsonl/truthfulqa-{model}_initial_iti_{k}_{strength}.0.jsonl', 'r') as file:\n",
    "        with open(f'evaluation_jsonl/truthfulqa-{model}_initial_iti_{k}_{strength}.0_mlp.jsonl', 'r') as file:\n",
    "        # with open(f'evaluation_jsonl/truthfulqa-{model}_initial_iti_{k}_{strength}.0_residual.jsonl', 'r') as file:\n",
    "        # with open(f'evaluation_jsonl/truthfulqa-{model}_initial_base.jsonl', 'r') as file:\n",
    "            for line in file:\n",
    "                json_object = json.loads(line.strip())\n",
    "                initial_predictions.append(json_object['response']['body']['choices'][0]['message']['content'])\n",
    "\n",
    "        final_predictions = []\n",
    "        # with open(f'evaluation_jsonl/mmlu-{model}_intervened_64_-20.0.jsonl', 'r') as file:\n",
    "        with open(f'evaluation_jsonl/truthfulqa-{model}_final_iti_{k}_{strength}.0_mlp.jsonl', 'r') as file:\n",
    "        # with open(f'evaluation_jsonl/truthfulqa-{model}_final_iti_{k}_{strength}.0_residual.jsonl', 'r') as file:\n",
    "        # with open(f'evaluation_jsonl/truthfulqa-{model}_final_base.jsonl', 'r') as file:\n",
    "            for line in file:\n",
    "                json_object = json.loads(line.strip())\n",
    "                final_predictions.append(json_object['response']['body']['choices'][0]['message']['content'])\n",
    "\n",
    "        initial_accuracy = compute_accuracy(initial_predictions)\n",
    "        current_k_initial_accuracies.append(initial_accuracy)\n",
    "        final_accuracy = compute_accuracy(final_predictions)\n",
    "        current_k_final_accuracies.append(final_accuracy)\n",
    "        \n",
    "        correct_to_incorrect_count = 0\n",
    "        initial_correct_count = 0\n",
    "        for y1, y2 in zip(initial_predictions, final_predictions):\n",
    "            if y1 == \"CORRECT\" and y2 == \"INCORRECT\":\n",
    "                correct_to_incorrect_count+=1\n",
    "            if y1 == \"CORRECT\":\n",
    "                initial_correct_count+=1\n",
    "        shift = correct_to_incorrect_count/initial_correct_count\n",
    "        current_k_shifts.append(shift)\n",
    "    initial_accuracies.append(current_k_initial_accuracies)\n",
    "    final_accuracies.append(current_k_final_accuracies)\n",
    "    shifts.append(current_k_shifts)\n",
    "\n",
    "initial_accuracies, final_accuracies, shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "questions_test = ds['validation']['question'][int(0.80*len(ds['validation'])):]\n",
    "correct_answers_test = ds['validation']['correct_answers'][int(0.80*len(ds['validation'])):]\n",
    "categories = ds['validation']['category'][int(0.80*len(ds['validation'])):]\n",
    "\n",
    "correct_and_total_counts = {} #key: category, value: (correct_count, total_count)\n",
    "for i, pred in enumerate(final_predictions):\n",
    "    current_category = categories[i]\n",
    "    if current_category in correct_and_total_counts:\n",
    "        correct_count, total_count = correct_and_total_counts[current_category]\n",
    "        if pred == \"CORRECT\":\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "        correct_and_total_counts[current_category] = (correct_count, total_count)\n",
    "    else:\n",
    "        total_count, correct_count = 1, 0\n",
    "        if pred == \"CORRECT\":\n",
    "            correct_count += 1\n",
    "        correct_and_total_counts[current_category] = (correct_count, total_count)\n",
    "intervened_accuracies = {key:value[0]/value[1] for key, value in correct_and_total_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions = []\n",
    "with open(f'evaluation_jsonl/truthfulqa-{model}_final_base.jsonl', 'r') as file:\n",
    "# with open(f'evaluation_jsonl/truthfulqa-{model}_final_base.jsonl', 'r') as file:\n",
    "    for line in file:\n",
    "        json_object = json.loads(line.strip())\n",
    "        base_predictions.append(json_object['response']['body']['choices'][0]['message']['content'])\n",
    "\n",
    "correct_and_total_counts = {} #key: category, value: (correct_count, total_count)\n",
    "for i, pred in enumerate(base_predictions):\n",
    "    current_category = categories[i]\n",
    "    if current_category in correct_and_total_counts:\n",
    "        correct_count, total_count = correct_and_total_counts[current_category]\n",
    "        if pred == \"CORRECT\":\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "        correct_and_total_counts[current_category] = (correct_count, total_count)\n",
    "    else:\n",
    "        total_count, correct_count = 1, 0\n",
    "        if pred == \"CORRECT\":\n",
    "            correct_count += 1\n",
    "        correct_and_total_counts[current_category] = (correct_count, total_count)\n",
    "\n",
    "base_accuracies = {key:value[0]/value[1] for key, value in correct_and_total_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Original dictionaries\n",
    "dict1 = base_accuracies\n",
    "dict2 = intervened_accuracies\n",
    "\n",
    "# Create a DataFrame for better handling\n",
    "df = pd.DataFrame({\n",
    "    'Base Model Accuracy': dict1,\n",
    "    'Intervened Model Accuracy': dict2\n",
    "}).reset_index().rename(columns={'index': 'Category'})\n",
    "\n",
    "# Sort by Base Model Accuracy values for better visualization\n",
    "df = df.sort_values(by='Base Model Accuracy', ascending=False)\n",
    "\n",
    "# Set up figure and axes with larger figure size\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Set width of bars\n",
    "barWidth = 0.35\n",
    "\n",
    "# Set positions of bars on X axis\n",
    "r1 = np.arange(len(df))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "\n",
    "# Create bars\n",
    "ax.bar(r1, df['Base Model Accuracy'], width=barWidth, edgecolor='grey', label='Base Model Accuracy', color='lightgrey')\n",
    "ax.bar(r2, df['Intervened Model Accuracy'], width=barWidth, edgecolor='grey', label='Intervened Model Accuracy', color='#FF9500')\n",
    "\n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('Category', fontweight='bold', fontsize=20)\n",
    "plt.ylabel('Accuracy', fontweight='bold', fontsize=20)\n",
    "plt.title('Final Accuracy Comparison by TruthfulQA Category', fontweight='bold', fontsize=24)\n",
    "plt.xticks([r + barWidth/2 for r in range(len(df))], df['Category'], rotation=75, fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# Ensure y-axis starts at 0 and ends at 1 for accuracy values\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend(fontsize=16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.savefig(\"accuracy_comparison_truthfulqa.pdf\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Sweep Heatmap for each metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Font sizes\n",
    "title_fontsize = 18\n",
    "axis_label_fontsize = 18\n",
    "xtick_fontsize = 14  # Customizable X-tick font size\n",
    "ytick_fontsize = 14  # Customizable Y-tick font size\n",
    "annotation_fontsize = 16 # For heatmap cell annotations\n",
    "colorbar_label_fontsize = 16 # For the colorbar label\n",
    "colorbar_tick_fontsize = 12 # For the colorbar ticks\n",
    "\n",
    "# Figure and Plot settings\n",
    "figure_width = 11 # Adjusted slightly to accommodate colorbar better\n",
    "figure_height = 9\n",
    "heatmap_cmap = \"Blues\"\n",
    "heatmap_vmin = 0.3\n",
    "heatmap_vmax = 0.6\n",
    "heatmap_fmt = \".3f\"\n",
    "colorbar_label_text = \"Initial Accuracy\"\n",
    "tight_layout_pad = 1.5 # Padding for tight_layout\n",
    "\n",
    "# Output file\n",
    "output_filename = \"initial_accuracy_mha_hyperparam.pdf\"\n",
    "# --- End Customizable Parameters ---\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(initial_accuracies, index=TOP_K_HEADS, columns=SCALES)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = sns.heatmap(df,\n",
    "                      annot=True,\n",
    "                      cmap=heatmap_cmap,\n",
    "                      vmin=heatmap_vmin,\n",
    "                      vmax=heatmap_vmax,\n",
    "                      annot_kws={\"size\": annotation_fontsize, \"weight\": \"normal\"}, # Adjusted weight\n",
    "                      fmt=heatmap_fmt,\n",
    "                      cbar=True,  # Ensure colorbar is present\n",
    "                      cbar_kws={'label': colorbar_label_text} # Add label to colorbar\n",
    "                     )\n",
    "\n",
    "# Customize colorbar label font size and tick font size\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.set_label(colorbar_label_text, fontsize=colorbar_label_fontsize)\n",
    "cbar.ax.tick_params(labelsize=colorbar_tick_fontsize)\n",
    "\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Effect of varying top k heads and intervention strength\\non initial accuracy for Gemma-3\",\n",
    "          fontsize=title_fontsize, pad=20) # Added some padding to title\n",
    "plt.xlabel(\"Intervention Strength\", fontsize=axis_label_fontsize)\n",
    "plt.ylabel(\"Top k heads\", fontsize=axis_label_fontsize)\n",
    "\n",
    "# Set tick label font size\n",
    "plt.xticks(fontsize=xtick_fontsize, rotation=45, ha=\"right\") # Added rotation for better readability if scales are long\n",
    "plt.yticks(fontsize=ytick_fontsize)\n",
    "\n",
    "# Adjust layout to make sure everything fits\n",
    "plt.tight_layout(pad=tight_layout_pad)\n",
    "\n",
    "# Save the plot to PDF\n",
    "# It's often good to save before plt.show()\n",
    "# bbox_inches='tight' ensures the saved figure includes all elements without extra whitespace\n",
    "plt.savefig(output_filename, bbox_inches='tight', dpi=300)\n",
    "print(f\"Plot saved to {output_filename}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Font sizes\n",
    "title_fontsize = 18\n",
    "axis_label_fontsize = 18\n",
    "xtick_fontsize = 14  # Customizable X-tick font size\n",
    "ytick_fontsize = 14  # Customizable Y-tick font size\n",
    "annotation_fontsize = 16 # For heatmap cell annotations\n",
    "colorbar_label_fontsize = 16 # For the colorbar label\n",
    "colorbar_tick_fontsize = 12 # For the colorbar ticks\n",
    "\n",
    "# Figure and Plot settings\n",
    "figure_width = 11 # Adjusted slightly to accommodate colorbar better\n",
    "figure_height = 9\n",
    "heatmap_cmap = \"Blues\"\n",
    "heatmap_vmin = 0.3\n",
    "heatmap_vmax = 0.6\n",
    "heatmap_fmt = \".3f\"\n",
    "colorbar_label_text = \"Final Accuracy\"\n",
    "tight_layout_pad = 1.5 # Padding for tight_layout\n",
    "\n",
    "# Output file\n",
    "output_filename = \"final_accuracy_mha_hyperparam.pdf\"\n",
    "# --- End Customizable Parameters ---\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(final_accuracies, index=TOP_K_HEADS, columns=SCALES)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = sns.heatmap(df,\n",
    "                      annot=True,\n",
    "                      cmap=heatmap_cmap,\n",
    "                      vmin=heatmap_vmin,\n",
    "                      vmax=heatmap_vmax,\n",
    "                      annot_kws={\"size\": annotation_fontsize, \"weight\": \"normal\"}, # Adjusted weight\n",
    "                      fmt=heatmap_fmt,\n",
    "                      cbar=True,  # Ensure colorbar is present\n",
    "                      cbar_kws={'label': colorbar_label_text} # Add label to colorbar\n",
    "                     )\n",
    "\n",
    "# Customize colorbar label font size and tick font size\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.set_label(colorbar_label_text, fontsize=colorbar_label_fontsize)\n",
    "cbar.ax.tick_params(labelsize=colorbar_tick_fontsize)\n",
    "\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Effect of varying top k heads and intervention strength\\non final accuracy for Gemma-3\",\n",
    "          fontsize=title_fontsize, pad=20) # Added some padding to title\n",
    "plt.xlabel(\"Intervention Strength\", fontsize=axis_label_fontsize)\n",
    "plt.ylabel(\"Top k heads\", fontsize=axis_label_fontsize)\n",
    "\n",
    "# Set tick label font size\n",
    "plt.xticks(fontsize=xtick_fontsize, rotation=45, ha=\"right\") # Added rotation for better readability if scales are long\n",
    "plt.yticks(fontsize=ytick_fontsize)\n",
    "\n",
    "# Adjust layout to make sure everything fits\n",
    "plt.tight_layout(pad=tight_layout_pad)\n",
    "\n",
    "# Save the plot to PDF\n",
    "# It's often good to save before plt.show()\n",
    "# bbox_inches='tight' ensures the saved figure includes all elements without extra whitespace\n",
    "plt.savefig(output_filename, bbox_inches='tight', dpi=300)\n",
    "print(f\"Plot saved to {output_filename}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Font sizes\n",
    "title_fontsize = 18\n",
    "axis_label_fontsize = 18\n",
    "xtick_fontsize = 14  # Customizable X-tick font size\n",
    "ytick_fontsize = 14  # Customizable Y-tick font size\n",
    "annotation_fontsize = 16 # For heatmap cell annotations\n",
    "colorbar_label_fontsize = 16 # For the colorbar label\n",
    "colorbar_tick_fontsize = 12 # For the colorbar ticks\n",
    "\n",
    "# Figure and Plot settings\n",
    "figure_width = 11 # Adjusted slightly to accommodate colorbar better\n",
    "figure_height = 9\n",
    "heatmap_cmap = \"Blues_r\"\n",
    "heatmap_vmin = 0.3\n",
    "heatmap_vmax = 0.6\n",
    "heatmap_fmt = \".3f\"\n",
    "colorbar_label_text = \"Shift to Incorrect\"\n",
    "tight_layout_pad = 1.5 # Padding for tight_layout\n",
    "\n",
    "# Output file\n",
    "output_filename = \"shift_to_incorrect_mha_hyperparam.pdf\"\n",
    "# --- End Customizable Parameters ---\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(shifts, index=TOP_K_HEADS, columns=SCALES)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = sns.heatmap(df,\n",
    "                      annot=True,\n",
    "                      cmap=heatmap_cmap,\n",
    "                      vmin=heatmap_vmin,\n",
    "                      vmax=heatmap_vmax,\n",
    "                      annot_kws={\"size\": annotation_fontsize, \"weight\": \"normal\"}, # Adjusted weight\n",
    "                      fmt=heatmap_fmt,\n",
    "                      cbar=True,  # Ensure colorbar is present\n",
    "                      cbar_kws={'label': colorbar_label_text} # Add label to colorbar\n",
    "                     )\n",
    "\n",
    "# Customize colorbar label font size and tick font size\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.set_label(colorbar_label_text, fontsize=colorbar_label_fontsize)\n",
    "cbar.ax.tick_params(labelsize=colorbar_tick_fontsize)\n",
    "\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Effect of varying top k heads and intervention strength\\non shift to incorrect rate for Gemma-3\",\n",
    "          fontsize=title_fontsize, pad=20) # Added some padding to title\n",
    "plt.xlabel(\"Intervention Strength\", fontsize=axis_label_fontsize)\n",
    "plt.ylabel(\"Top k heads\", fontsize=axis_label_fontsize)\n",
    "\n",
    "# Set tick label font size\n",
    "plt.xticks(fontsize=xtick_fontsize, rotation=45, ha=\"right\") # Added rotation for better readability if scales are long\n",
    "plt.yticks(fontsize=ytick_fontsize)\n",
    "\n",
    "# Adjust layout to make sure everything fits\n",
    "plt.tight_layout(pad=tight_layout_pad)\n",
    "\n",
    "# Save the plot to PDF\n",
    "# It's often good to save before plt.show()\n",
    "# bbox_inches='tight' ensures the saved figure includes all elements without extra whitespace\n",
    "plt.savefig(output_filename, bbox_inches='tight', dpi=300)\n",
    "print(f\"Plot saved to {output_filename}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Font sizes\n",
    "title_fontsize = 18\n",
    "axis_label_fontsize = 18\n",
    "xtick_fontsize = 14  # Customizable X-tick font size\n",
    "ytick_fontsize = 14  # Customizable Y-tick font size\n",
    "annotation_fontsize = 16 # For heatmap cell annotations\n",
    "colorbar_label_fontsize = 16 # For the colorbar label\n",
    "colorbar_tick_fontsize = 12 # For the colorbar ticks\n",
    "\n",
    "# Figure and Plot settings\n",
    "figure_width = 11 # Adjusted slightly to accommodate colorbar better\n",
    "figure_height = 9\n",
    "heatmap_cmap = \"Blues\"\n",
    "heatmap_vmin = 0\n",
    "heatmap_vmax = 0.25\n",
    "heatmap_fmt = \".3f\"\n",
    "colorbar_label_text = \"KL Divergence\"\n",
    "tight_layout_pad = 1.5 # Padding for tight_layout\n",
    "\n",
    "# Output file\n",
    "output_filename = \"kl_divergences_mha_hyperparam.pdf\"\n",
    "# --- End Customizable Parameters ---\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(kl_divergences, index=TOP_K_HEADS, columns=SCALES)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(figure_width, figure_height))\n",
    "\n",
    "# Create the heatmap\n",
    "heatmap = sns.heatmap(df,\n",
    "                      annot=True,\n",
    "                      cmap=heatmap_cmap,\n",
    "                      vmin=heatmap_vmin,\n",
    "                      vmax=heatmap_vmax,\n",
    "                      annot_kws={\"size\": annotation_fontsize, \"weight\": \"normal\"}, # Adjusted weight\n",
    "                      fmt=heatmap_fmt,\n",
    "                      cbar=True,  # Ensure colorbar is present\n",
    "                      cbar_kws={'label': colorbar_label_text} # Add label to colorbar\n",
    "                     )\n",
    "\n",
    "# Customize colorbar label font size and tick font size\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.set_label(colorbar_label_text, fontsize=colorbar_label_fontsize)\n",
    "cbar.ax.tick_params(labelsize=colorbar_tick_fontsize)\n",
    "\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Effect of varying top k heads and intervention strength\\non KL Divergences for Gemma-3\",\n",
    "          fontsize=title_fontsize, pad=20) # Added some padding to title\n",
    "plt.xlabel(\"Intervention Strength\", fontsize=axis_label_fontsize)\n",
    "plt.ylabel(\"Top k heads\", fontsize=axis_label_fontsize)\n",
    "\n",
    "# Set tick label font size\n",
    "plt.xticks(fontsize=xtick_fontsize, rotation=45, ha=\"right\") # Added rotation for better readability if scales are long\n",
    "plt.yticks(fontsize=ytick_fontsize)\n",
    "\n",
    "# Adjust layout to make sure everything fits\n",
    "plt.tight_layout(pad=tight_layout_pad)\n",
    "\n",
    "# Save the plot to PDF\n",
    "# It's often good to save before plt.show()\n",
    "# bbox_inches='tight' ensures the saved figure includes all elements without extra whitespace\n",
    "plt.savefig(output_filename, bbox_inches='tight', dpi=300)\n",
    "print(f\"Plot saved to {output_filename}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycophancy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
