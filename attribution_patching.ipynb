{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-41): 42 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
      "          (rotary_emb): Gemma2RotaryEmbedding()\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import einops\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected+colab+notebook\"\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import os \n",
    "os.environ['HF_TOKEN'] = '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'\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model_name = \"google/gemma-2-9b\"\n",
    "# model_name = \"openai-community/gpt2\"\n",
    "# model = LanguageModel(model_name, device_map=\"auto\", dispatch=True)\n",
    "model = LanguageModel(model_name, device_map=\"auto\", dispatch=True, quantization_config=quantization_config)\n",
    "clear_output()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"When John and Mary went to the shops, John gave the bag to\",\n",
    "    \"When John and Mary went to the shops, Mary gave the bag to\",\n",
    "    \"When Tom and James went to the park, James gave the ball to\",\n",
    "    \"When Tom and James went to the park, Tom gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops, Sid gave an apple to\",\n",
    "    \"When Dan and Sid went to the shops, Dan gave an apple to\",\n",
    "    \"After Martin and Amy went to the park, Amy gave a drink to\",\n",
    "    \"After Martin and Amy went to the park, Martin gave a drink to\",\n",
    "]\n",
    "\n",
    "# Answers are each formatted as (correct, incorrect):\n",
    "answers = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" John\", \" Mary\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" James\", \" Tom\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Sid\", \" Dan\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "    (\" Amy\", \" Martin\"),\n",
    "]\n",
    "\n",
    "# Tokenize clean and corrupted inputs:\n",
    "clean_tokens = model.tokenizer(prompts, return_tensors=\"pt\")[\"input_ids\"]\n",
    "# The associated corrupted input is the prompt after the current clean prompt\n",
    "# for even indices, or the prompt prior to the current clean prompt for odd indices\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i + 1 if i % 2 == 0 else i - 1) for i in range(len(clean_tokens))]\n",
    "]\n",
    "\n",
    "# Tokenize answers for each prompt:\n",
    "answer_token_indices = torch.tensor(\n",
    "    [\n",
    "        [model.tokenizer(answers[i][j])[\"input_ids\"][0] for j in range(2)]\n",
    "        for i in range(len(answers))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    logits = logits[:, -1, :]\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.decode(model(clean_tokens.to(model.device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits = model.trace(clean_tokens, trace=False).logits.cpu()\n",
    "corrupted_logits = model.trace(corrupted_tokens, trace=False).logits.cpu()\n",
    "\n",
    "CLEAN_BASELINE = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "print(f\"Clean logit diff: {CLEAN_BASELINE:.4f}\")\n",
    "\n",
    "CORRUPTED_BASELINE = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "print(f\"Corrupted logit diff: {CORRUPTED_BASELINE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_metric(\n",
    "    logits,\n",
    "    answer_token_indices=answer_token_indices,\n",
    "):\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
    "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
    "    )\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_metric(\n",
    "    logits,\n",
    "    answer_token_indices=answer_token_indices,\n",
    "):\n",
    "    return (get_logit_diff(logits, answer_token_indices) - CORRUPTED_BASELINE) / (\n",
    "        CLEAN_BASELINE - CORRUPTED_BASELINE\n",
    "    )\n",
    "\n",
    "print(f\"Clean Baseline is 1: {ioi_metric(clean_logits).item():.4f}\")\n",
    "print(f\"Corrupted Baseline is 0: {ioi_metric(corrupted_logits).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycophancy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
