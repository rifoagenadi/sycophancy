{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "import pickle\n",
    "import torch\n",
    "import pyvene as pv\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import importlib\n",
    "import utils\n",
    "from utils import generate_and_decode_new_tokens, to_request, create_anthropic_batch_job\n",
    "from inference import get_probe_vectors, get_top_k_keys\n",
    "model_id = 'gemma-3'\n",
    "k_heads = 32\n",
    "scale = -20\n",
    "\n",
    "\n",
    "model, processor = load_model(model_id)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading accuracies for model: {model_id}\")\n",
    "accuracies = pickle.load(open(f'linear_probe/trained_probe/{model_id}/accuracies_dict.pkl', 'rb'))\n",
    "config = model.config\n",
    "\n",
    "# Set model parameters based on model type\n",
    "if 'gemma' in str(type(model)).lower():\n",
    "    NUM_LAYERS = model.config.text_config.num_hidden_layers\n",
    "    HIDDEN_DIM = model.config.text_config.hidden_size\n",
    "    NUM_HEADS = model.config.text_config.num_attention_heads\n",
    "    HEAD_DIM = model.config.text_config.head_dim\n",
    "else:\n",
    "    NUM_LAYERS = model.config.num_hidden_layers\n",
    "    HIDDEN_DIM = model.config.hidden_size\n",
    "    NUM_HEADS = model.config.num_attention_heads\n",
    "    HEAD_DIM = model.config.head_dim\n",
    "\n",
    "print(f\"Getting top {k_heads} heads\")\n",
    "top_k_heads = get_top_k_keys(accuracies, k_heads)\n",
    "print(f\"Top heads selected: {top_k_heads}\")\n",
    "\n",
    "print(f\"Creating probe vectors with scale {scale}\")\n",
    "linear_probes = get_probe_vectors(top_k_heads, model_id, scale=scale, \n",
    "                                    num_layers=NUM_LAYERS, head_dim=HEAD_DIM, num_heads=NUM_HEADS,\n",
    "                                    # use_random_direction=True\n",
    "                                    )\n",
    "\n",
    "print(\"Setting up intervention components\")\n",
    "if model_id == 'gemma-3':\n",
    "    target_components = [{\n",
    "            \"component\": f\"language_model.model.layers[{i}].self_attn.o_proj.input\",\n",
    "            \"intervention\": pv.AdditionIntervention(\n",
    "                source_representation=linear_probes[i].to(\"cuda\")\n",
    "            )\n",
    "            # \"intervention\": pv.ZeroIntervention\n",
    "        } for i in range(NUM_LAYERS) if torch.count_nonzero(linear_probes[i])]\n",
    "else:\n",
    "    target_components = [{\n",
    "            \"component\": f\"model.layers[{i}].self_attn.o_proj.input\",\n",
    "            \"intervention\": pv.AdditionIntervention(\n",
    "                source_representation=linear_probes[i].to(\"cuda\")\n",
    "            )\n",
    "            # \"intervention\": pv.ZeroIntervention\n",
    "        } for i in range(NUM_LAYERS) if torch.count_nonzero(linear_probes[i])]\n",
    "\n",
    "print(\"Creating interventable model\")\n",
    "pv_model = pv.IntervenableModel(target_components, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "questions_test = ds['validation']['question'][int(0.80*len(ds['validation'])):]\n",
    "correct_answers_test = ds['validation']['correct_answers'][int(0.80*len(ds['validation'])):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "import pickle\n",
    "import torch\n",
    "import pyvene as pv\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import importlib\n",
    "import utils\n",
    "from utils import generate_and_decode_new_tokens, to_request, create_anthropic_batch_job\n",
    "from inference import get_probe_vectors, get_top_k_keys\n",
    "model_id = 'gemma-3'\n",
    "k_heads = 32\n",
    "scale = -20\n",
    "\n",
    "list_k_heads = [32, 16]\n",
    "scales=[-20, -10, 10, 20]\n",
    "\n",
    "model, processor = load_model(model_id)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loading accuracies for model: {model_id}\")\n",
    "accuracies = pickle.load(open(f'linear_probe/trained_probe/{model_id}/accuracies_dict.pkl', 'rb'))\n",
    "config = model.config\n",
    "\n",
    "for k_heads in list_k_heads:\n",
    "    for scale in scales:\n",
    "        # Set model parameters based on model type\n",
    "        if 'gemma' in str(type(model)).lower():\n",
    "            NUM_LAYERS = model.config.text_config.num_hidden_layers\n",
    "            HIDDEN_DIM = model.config.text_config.hidden_size\n",
    "            NUM_HEADS = model.config.text_config.num_attention_heads\n",
    "            HEAD_DIM = model.config.text_config.head_dim\n",
    "        else:\n",
    "            NUM_LAYERS = model.config.num_hidden_layers\n",
    "            HIDDEN_DIM = model.config.hidden_size\n",
    "            NUM_HEADS = model.config.num_attention_heads\n",
    "            HEAD_DIM = model.config.head_dim\n",
    "\n",
    "        print(f\"Getting top {k_heads} heads\")\n",
    "        top_k_heads = get_top_k_keys(accuracies, k_heads)\n",
    "        print(f\"Top heads selected: {top_k_heads}\")\n",
    "\n",
    "        print(f\"Creating probe vectors with scale {scale}\")\n",
    "        linear_probes = get_probe_vectors(top_k_heads, model_id, scale=scale, \n",
    "                                            num_layers=NUM_LAYERS, head_dim=HEAD_DIM, num_heads=NUM_HEADS,\n",
    "                                            # use_random_direction=True\n",
    "                                            )\n",
    "\n",
    "        print(\"Setting up intervention components\")\n",
    "        if model_id == 'gemma-3':\n",
    "            target_components = [{\n",
    "                    \"component\": f\"language_model.model.layers[{i}].self_attn.o_proj.input\",\n",
    "                    \"intervention\": pv.AdditionIntervention(\n",
    "                        source_representation=linear_probes[i].to(\"cuda\")\n",
    "                    )\n",
    "                    # \"intervention\": pv.ZeroIntervention\n",
    "                } for i in range(NUM_LAYERS) if torch.count_nonzero(linear_probes[i])]\n",
    "        else:\n",
    "            target_components = [{\n",
    "                    \"component\": f\"model.layers[{i}].self_attn.o_proj.input\",\n",
    "                    \"intervention\": pv.AdditionIntervention(\n",
    "                        source_representation=linear_probes[i].to(\"cuda\")\n",
    "                    )\n",
    "                    # \"intervention\": pv.ZeroIntervention\n",
    "                } for i in range(NUM_LAYERS) if torch.count_nonzero(linear_probes[i])]\n",
    "\n",
    "        print(\"Creating interventable model\")\n",
    "        pv_model = pv.IntervenableModel(target_components, model=model)\n",
    "\n",
    "        system_prompt = \"You are a helpful assistant.\"\n",
    "        import torch\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        # Assuming orig_outputs and intervened_outputs are logits\n",
    "        def compute_kl_divergence(original_logits, intervened_logits):\n",
    "            # Convert logits to probabilities using softmax\n",
    "            original_probs = F.softmax(original_logits, dim=-1)\n",
    "            intervened_probs = F.softmax(intervened_logits, dim=-1)\n",
    "            \n",
    "            # Calculate KL divergence: KL(original || intervened)\n",
    "            # We add a small epsilon to avoid log(0)\n",
    "            epsilon = 1e-10\n",
    "            kl_div = torch.sum(original_probs * torch.log((original_probs + epsilon) / (intervened_probs + epsilon)), dim=-1)\n",
    "            \n",
    "            # Return mean KL divergence across all tokens/positions\n",
    "            return kl_div.mean()\n",
    "\n",
    "        kl_divergences = []\n",
    "        for prompt in questions_test:\n",
    "            if model_id == 'gemma-3':\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt + \" Give me your best guess and answer as concisely as possible.\"}\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            else:\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt + \" Give me your best guess and answer as concisely as possible.\"\n",
    "                    }\n",
    "                ]\n",
    "            q = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "            orig_outputs, intervened_outputs = pv_model(\n",
    "                processor.tokenizer(q, return_tensors=\"pt\").to('cuda'),\n",
    "                output_original_output=True\n",
    "            )\n",
    "            kl_divergence = compute_kl_divergence(orig_outputs.logits, intervened_outputs.logits).item()\n",
    "            kl_divergences.append(kl_divergence)\n",
    "\n",
    "        print(f\"{k_heads = }, {scale = }, {sum(kl_divergences)/len(kl_divergences) = }\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming orig_outputs and intervened_outputs are logits\n",
    "def compute_kl_divergence(original_logits, intervened_logits):\n",
    "    # Convert logits to probabilities using softmax\n",
    "    original_probs = F.softmax(original_logits, dim=-1)\n",
    "    intervened_probs = F.softmax(intervened_logits, dim=-1)\n",
    "    \n",
    "    # Calculate KL divergence: KL(original || intervened)\n",
    "    # We add a small epsilon to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    kl_div = torch.sum(original_probs * torch.log((original_probs + epsilon) / (intervened_probs + epsilon)), dim=-1)\n",
    "    \n",
    "    # Return mean KL divergence across all tokens/positions\n",
    "    return kl_div.mean()\n",
    "\n",
    "kl_divergences = []\n",
    "for prompt in questions_test:\n",
    "   if model_id == 'gemma-3':\n",
    "      messages = [\n",
    "         {\n",
    "               \"role\": \"system\",\n",
    "               \"content\": [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "         },\n",
    "         {\n",
    "               \"role\": \"user\",\n",
    "               \"content\": [\n",
    "                  {\"type\": \"text\", \"text\": prompt + \" Give me your best guess and answer as concisely as possible.\"}\n",
    "               ]\n",
    "         }\n",
    "      ]\n",
    "   else:\n",
    "      messages = [\n",
    "         {\n",
    "               \"role\": \"system\",\n",
    "               \"content\": system_prompt\n",
    "         },\n",
    "         {\n",
    "               \"role\": \"user\",\n",
    "               \"content\": prompt + \" Give me your best guess and answer as concisely as possible.\"\n",
    "         }\n",
    "      ]\n",
    "   q = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "   orig_outputs, intervened_outputs = pv_model(\n",
    "      processor.tokenizer(q, return_tensors=\"pt\").to('cuda'),\n",
    "      output_original_output=True\n",
    "   )\n",
    "   kl_divergence = compute_kl_divergence(orig_outputs.logits, intervened_outputs.logits).item()\n",
    "   kl_divergences.append(kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(kl_divergences)/len(kl_divergences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idxs = [12, 16, 17, 20, 34, 41, 55, 58, 65, 70, 74, 75, 95, 96, 101, 106, 109, 110, 120, 127, 129, 136, 146, 151, 152, 158, 163]\n",
    "\n",
    "questions = []\n",
    "correct_answers = []\n",
    "initial_answers = []\n",
    "final_answers = []\n",
    "from tqdm.auto import tqdm\n",
    "for idx in tqdm(sample_idxs[:10]):\n",
    "    question = questions_test[idx]\n",
    "    correct_answer = correct_answers_test[idx]\n",
    "    questions.append(question)\n",
    "    correct_answers.append(correct_answer)\n",
    "\n",
    "    res_1, res_2 = generate_and_decode_new_tokens(question, pv_model, processor, model_id)\n",
    "    initial_answers.append(res_1)\n",
    "    final_answers.append(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'questions': questions, 'correct_answers': correct_answers, 'initial_answers': initial_answers, 'final_answers': final_answers}).to_csv('qualitative.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycophancy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
