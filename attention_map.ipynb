{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "\n",
    "model, processor = load_model('llama-3.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "accuracies = pickle.load(open('/home/rifo.genadi/Documents/sycophancy_exploration/linear_probe/trained_probe/Qwen3-4B-Instruct-2507/accuracies_dict_mha.pkl', 'rb'))\n",
    "# accuracies = pickle.load(open('/home/rifo.genadi/Documents/sycophancy_exploration/linear_probe/trained_probe/Llama-3.2-3B-Instruct/accuracies_dict_mha.pkl', 'rb'))\n",
    "# accuracies = pickle.load(open('/home/rifo.genadi/Documents/sycophancy_exploration/linear_probe/trained_probe/gemma-3-4b-it/accuracies_dict_mha.pkl', 'rb'))\n",
    "items = accuracies.items()\n",
    "sorted_items = sorted(items, key=lambda item: item[1], reverse=True)\n",
    "top_64 = sorted_items[:64]\n",
    "import random\n",
    "least_64 = sorted_items[-64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_probe.linear_probe_data_utils import construct_data\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import importlib\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"Loading and preparing TruthfulQA data...\")\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "ds_train_val = ds['validation'] # Use the full validation set for train/val split\n",
    "ds_train_split = ds_train_val[:int(0.8*len(ds_train_val))] # 80% for training activation extraction/probe training\n",
    "# ds_val_split = ds_train_val[int(0.8*len(ds_train_val)):] # 20% held out? The notebooks used same split for extraction and training\n",
    "\n",
    "chats, labels = construct_data(ds_train_split, model='llama') # Simple model name\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "numerical_labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Applying chat template and tokenizing...\")\n",
    "chats_templated = tokenizer.apply_chat_template(chats, add_generation_prompt=False, tokenize=False)\n",
    "tokenized_data = [\n",
    "    tokenizer(text=chat, return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
    "    for chat in tqdm(chats_templated, desc=\"Tokenizing\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_attention(\n",
    "    all_layer_attentions,\n",
    "    layer_idx,\n",
    "    head_idx,\n",
    "    tokens,\n",
    "    batch_idx=0,\n",
    "    figsize=(10, 8),\n",
    "    cmap=\"Reds\",\n",
    "    token_groups=None,\n",
    "    group_aggregation_method=\"mean\",\n",
    "    normalization=\"row_percent\"   # \"row_percent\" | \"softmax\" | \"none\"\n",
    "):\n",
    "    def _softmax(x, axis=-1):\n",
    "        x = x - np.max(x, axis=axis, keepdims=True)  # stability\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "    def _row_percent(x, axis=1):\n",
    "        \"\"\"\n",
    "        Normalize each row to percentages that sum to 100.\n",
    "        Zero-sum rows stay all-zeros.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        row_sums = np.sum(x, axis=axis, keepdims=True)          # shape (n_rows, 1)\n",
    "        out = np.zeros_like(x, dtype=np.float32)\n",
    "        # Broadcasted, safe divide: only divide where row_sums != 0\n",
    "        np.divide(x, row_sums, out=out, where=(row_sums != 0))\n",
    "        out *= 100.0\n",
    "        return out\n",
    "\n",
    "    if not all_layer_attentions:\n",
    "        print(\"Error: all_layer_attentions is empty or None.\")\n",
    "        return\n",
    "\n",
    "    if layer_idx < 0 or layer_idx >= len(all_layer_attentions):\n",
    "        print(f\"Error: layer_idx {layer_idx} is out of bounds (0-{len(all_layer_attentions)-1}).\")\n",
    "        return\n",
    "\n",
    "    attentions_for_layer = all_layer_attentions[layer_idx]\n",
    "\n",
    "    if batch_idx < 0 or batch_idx >= attentions_for_layer.shape[0]:\n",
    "        print(f\"Error: batch_idx {batch_idx} is out of bounds (0-{attentions_for_layer.shape[0]-1}).\")\n",
    "        return\n",
    "\n",
    "    num_heads = attentions_for_layer.shape[1]\n",
    "    if head_idx < 0 or head_idx >= num_heads:\n",
    "        print(f\"Error: head_idx {head_idx} is out of bounds (0-{num_heads-1}).\")\n",
    "        return\n",
    "\n",
    "    token_attention_matrix = (\n",
    "        attentions_for_layer[batch_idx, head_idx]\n",
    "        .detach().cpu().to(torch.float32).numpy()\n",
    "    )\n",
    "    original_seq_len = token_attention_matrix.shape[0]\n",
    "\n",
    "    if len(tokens) != original_seq_len:\n",
    "         print(f\"Warning: Number of tokens ({len(tokens)}) != sequence length in matrix ({original_seq_len}).\")\n",
    "\n",
    "    plot_attention_matrix = token_attention_matrix\n",
    "    effective_tokens_x = tokens\n",
    "    effective_tokens_y = tokens\n",
    "    title_suffix = \"\"\n",
    "\n",
    "    if token_groups:\n",
    "        num_groups = len(token_groups)\n",
    "        grouped_attention_matrix = np.zeros((num_groups, num_groups), dtype=np.float32)\n",
    "        group_names = [name for _, _, name in token_groups]\n",
    "        title_suffix = \" (Grouped)\"\n",
    "\n",
    "        max_token_idx_in_groups = max(end_idx for _, end_idx, _ in token_groups)\n",
    "        if max_token_idx_in_groups > original_seq_len:\n",
    "            print(\"Error: Token groups indices exceed sequence length.\")\n",
    "            return\n",
    "        elif max_token_idx_in_groups < original_seq_len:\n",
    "            print(\"Warning: Token groups cover fewer tokens than sequence length.\")\n",
    "\n",
    "        for i, (q_start, q_end, _) in enumerate(token_groups):\n",
    "            for j, (k_start, k_end, _) in enumerate(token_groups):\n",
    "                q_end_eff = min(q_end, original_seq_len)\n",
    "                k_end_eff = min(k_end, original_seq_len)\n",
    "                q_start_eff = min(q_start, original_seq_len)\n",
    "                k_start_eff = min(k_start, original_seq_len)\n",
    "\n",
    "                if q_start_eff >= q_end_eff or k_start_eff >= k_end_eff:\n",
    "                    attention_sum = 0.0\n",
    "                    num_vals = 1\n",
    "                else:\n",
    "                    sub_matrix = token_attention_matrix[q_start_eff:q_end_eff, k_start_eff:k_end_eff]\n",
    "                    attention_sum = np.sum(sub_matrix)\n",
    "                    num_vals = sub_matrix.size if sub_matrix.size > 0 else 1\n",
    "\n",
    "                if group_aggregation_method == \"mean\":\n",
    "                    grouped_attention_matrix[i, j] = attention_sum / num_vals\n",
    "                elif group_aggregation_method == \"sum\":\n",
    "                    grouped_attention_matrix[i, j] = attention_sum\n",
    "                else:\n",
    "                    raise ValueError(\"group_aggregation_method must be 'sum' or 'mean'\")\n",
    "\n",
    "        plot_attention_matrix = grouped_attention_matrix\n",
    "        effective_tokens_x = group_names\n",
    "        effective_tokens_y = group_names\n",
    "\n",
    "    # --- Normalization choice ---\n",
    "    if normalization == \"row_percent\":\n",
    "        plot_attention_matrix = _row_percent(plot_attention_matrix, axis=1)\n",
    "        cbar_label = \"Row % (each row sums to 100)\"\n",
    "        vmin, vmax = 0, 100\n",
    "    elif normalization == \"softmax\":\n",
    "        plot_attention_matrix = _softmax(plot_attention_matrix, axis=1)\n",
    "        cbar_label = \"Probability (softmax over keys)\"\n",
    "        vmin, vmax = None, None\n",
    "    elif normalization == \"none\":\n",
    "        cbar_label = \"Attention Score\"\n",
    "        vmin, vmax = None, None\n",
    "    else:\n",
    "        raise ValueError(\"normalization must be 'row_percent', 'softmax', or 'none'\")\n",
    "\n",
    "    # Plot\n",
    "    # fig, ax = plt.subplots(figsize=figsize)\n",
    "    # im = ax.imshow(plot_attention_matrix, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # ax.set_xticks(np.arange(len(effective_tokens_x)))\n",
    "    # ax.set_yticks(np.arange(len(effective_tokens_y)))\n",
    "\n",
    "    # ax.set_xticklabels(effective_tokens_x, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # ax.set_yticklabels(effective_tokens_y)\n",
    "\n",
    "    # cbar = fig.colorbar(im, ax=ax)\n",
    "    # cbar.set_label(cbar_label)\n",
    "\n",
    "    # ax.set_xlabel(\"Key Tokens/Groups (Attended To)\")\n",
    "    # ax.set_ylabel(\"Query Tokens/Groups (Attending From)\")\n",
    "    # ax.set_title(f\"Attention - Layer {layer_idx}, Head {head_idx} (Batch {batch_idx}){title_suffix}\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"attention_vis/top_head/attention_visualization_{layer_idx}_{head_idx}.pdf\", dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "    return plot_attention_matrix\n",
    "\n",
    "\n",
    "def get_groups(tokens):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples, where each tuple is (start_idx, end_idx (exclusive), group_name).\n",
    "    \"\"\"\n",
    "    token_groups = []\n",
    "\n",
    "    # get bos group start and end token\n",
    "    bos_start = tokens.index(\"<|begin_of_text|>\")\n",
    "    bos_end = tokens.index(\"<|start_header_id|>\")\n",
    "    token_groups.append((bos_start, bos_end, \"BOS\"))\n",
    "\n",
    "    # get system prompt group start and end token\n",
    "    sys_prompt_start = bos_end\n",
    "    sys_prompt_end = tokens.index(\"<|start_header_id|>\", sys_prompt_start+1)\n",
    "    token_groups.append((sys_prompt_start, sys_prompt_end, \"System Prompt\"))\n",
    "\n",
    "    # get user question group start and end token\n",
    "    user_q_start = sys_prompt_end\n",
    "    user_q_end = tokens.index(\"?\", user_q_start)+1\n",
    "    token_groups.append((user_q_start, user_q_end, \"Question\"))\n",
    "\n",
    "    # get user hypothesis group start and end token\n",
    "    user_hyp_start = user_q_end\n",
    "    user_hyp_end = tokens.index('Ġbut')\n",
    "    token_groups.append((user_hyp_start, user_hyp_end, \"User Hypothesis\"))\n",
    "\n",
    "    # get user doubt group start and end token\n",
    "    user_doubt_start = user_hyp_end\n",
    "    user_doubt_end = tokens.index(\"<|eot_id|>\", user_doubt_start)+1\n",
    "    token_groups.append((user_doubt_start, user_doubt_end, \"User Doubt\"))\n",
    "\n",
    "    # get model answer group start and end token\n",
    "    model_ans_start = tokens.index(\"<|start_header_id|>\", user_doubt_end)\n",
    "    model_ans_end = tokens.index(\"<|eot_id|>\", model_ans_start)+1\n",
    "    token_groups.append((model_ans_start, model_ans_end, \"Model Answer\"))\n",
    "    return token_groups[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "ans_to_doubt_scores = []\n",
    "ans_to_hypothesis_scores = []\n",
    "for key, value in tqdm(sorted_items):\n",
    "    ans_to_doubt = []\n",
    "    ans_to_hypothesis = []\n",
    "    for num in nums:\n",
    "        layer, head = key.split(\"_\")\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=tokenized_data[num].to('cuda').unsqueeze(0), output_attentions=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_data[num].tolist()) # For the first (and only) batch item\n",
    "        token_groups_definition = get_groups(tokens)\n",
    "        matrix = visualize_attention(\n",
    "            all_layer_attentions=output.attentions,\n",
    "            layer_idx=int(layer),\n",
    "            head_idx=int(head),\n",
    "            tokens=tokens,\n",
    "            batch_idx=0,\n",
    "            token_groups=token_groups_definition,\n",
    "            group_aggregation_method=\"sum\",\n",
    "            figsize=(8,6)\n",
    "        )\n",
    "        normalization = 'none' # 'row_percent' | 'softmax' | 'none'\n",
    "        ans_to_doubt.append(matrix[-1, -2])\n",
    "        ans_to_hypothesis.append(matrix[-1, -3])\n",
    "    ans_to_doubt_scores.append(np.mean(ans_to_doubt))\n",
    "    ans_to_hypothesis_scores.append(np.mean(ans_to_hypothesis))\n",
    "\n",
    "sum_scores = np.array(ans_to_doubt_scores) + np.array(ans_to_hypothesis_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accuracies = [x[1] for x in sorted_items]\n",
    "import numpy as np\n",
    "\n",
    "r = np.corrcoef(probe_accuracies, sum_scores)[0, 1]\n",
    "print(r)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(probe_accuracies, sum_scores, color='blue', alpha=0.7, edgecolors='k')  # points\n",
    "plt.xlabel(\"Probe Accurcies\")\n",
    "plt.ylabel(\"Sum of Attention Scores (Answer to Doubt + Answer to Hypothesis)\")\n",
    "plt.title(\"Scatter Plot of A vs. B\")\n",
    "\n",
    "# Optional: add a trendline\n",
    "import numpy as np\n",
    "m, c = np.polyfit(probe_accuracies, sum_scores, 1)  # slope & intercept\n",
    "plt.plot(probe_accuracies, np.array(probe_accuracies)*m + c, color='red', linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from transformers import AutoModel, AutoTokenizer # Keep if you need to run a model\n",
    "\n",
    "def visualize_attention(\n",
    "    all_layer_attentions,\n",
    "    layer_idx,\n",
    "    head_idx,\n",
    "    tokens,\n",
    "    batch_idx=0,\n",
    "    figsize=(10, 8),\n",
    "    cmap=\"Reds\",\n",
    "    token_groups=None,\n",
    "    group_aggregation_method=\"mean\",\n",
    "    normalization=\"row_percent\"   # \"row_percent\" | \"softmax\" | \"none\"\n",
    "):\n",
    "    def _softmax(x, axis=-1):\n",
    "        x = x - np.max(x, axis=axis, keepdims=True)  # stability\n",
    "        e = np.exp(x)\n",
    "        return e / np.sum(e, axis=axis, keepdims=True)\n",
    "\n",
    "    def _row_percent(x, axis=1):\n",
    "        \"\"\"\n",
    "        Normalize each row to percentages that sum to 100.\n",
    "        Zero-sum rows stay all-zeros.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x, dtype=np.float32)\n",
    "        row_sums = np.sum(x, axis=axis, keepdims=True)          # shape (n_rows, 1)\n",
    "        out = np.zeros_like(x, dtype=np.float32)\n",
    "        # Broadcasted, safe divide: only divide where row_sums != 0\n",
    "        np.divide(x, row_sums, out=out, where=(row_sums != 0))\n",
    "        out *= 100.0\n",
    "        return out\n",
    "\n",
    "    if not all_layer_attentions:\n",
    "        print(\"Error: all_layer_attentions is empty or None.\")\n",
    "        return\n",
    "\n",
    "    if layer_idx < 0 or layer_idx >= len(all_layer_attentions):\n",
    "        print(f\"Error: layer_idx {layer_idx} is out of bounds (0-{len(all_layer_attentions)-1}).\")\n",
    "        return\n",
    "\n",
    "    attentions_for_layer = all_layer_attentions[layer_idx]\n",
    "\n",
    "    if batch_idx < 0 or batch_idx >= attentions_for_layer.shape[0]:\n",
    "        print(f\"Error: batch_idx {batch_idx} is out of bounds (0-{attentions_for_layer.shape[0]-1}).\")\n",
    "        return\n",
    "\n",
    "    num_heads = attentions_for_layer.shape[1]\n",
    "    if head_idx < 0 or head_idx >= num_heads:\n",
    "        print(f\"Error: head_idx {head_idx} is out of bounds (0-{num_heads-1}).\")\n",
    "        return\n",
    "\n",
    "    token_attention_matrix = (\n",
    "        attentions_for_layer[batch_idx, head_idx]\n",
    "        .detach().cpu().to(torch.float32).numpy()\n",
    "    )\n",
    "    original_seq_len = token_attention_matrix.shape[0]\n",
    "\n",
    "    if len(tokens) != original_seq_len:\n",
    "         print(f\"Warning: Number of tokens ({len(tokens)}) != sequence length in matrix ({original_seq_len}).\")\n",
    "\n",
    "    plot_attention_matrix = token_attention_matrix\n",
    "    effective_tokens_x = tokens\n",
    "    effective_tokens_y = tokens\n",
    "    title_suffix = \"\"\n",
    "\n",
    "    if token_groups:\n",
    "        num_groups = len(token_groups)\n",
    "        grouped_attention_matrix = np.zeros((num_groups, num_groups), dtype=np.float32)\n",
    "        group_names = [name for _, _, name in token_groups]\n",
    "        title_suffix = \" (Grouped)\"\n",
    "\n",
    "        max_token_idx_in_groups = max(end_idx for _, end_idx, _ in token_groups)\n",
    "        if max_token_idx_in_groups > original_seq_len:\n",
    "            print(\"Error: Token groups indices exceed sequence length.\")\n",
    "            return\n",
    "        elif max_token_idx_in_groups < original_seq_len:\n",
    "            print(\"Warning: Token groups cover fewer tokens than sequence length.\")\n",
    "\n",
    "        for i, (q_start, q_end, _) in enumerate(token_groups):\n",
    "            for j, (k_start, k_end, _) in enumerate(token_groups):\n",
    "                q_end_eff = min(q_end, original_seq_len)\n",
    "                k_end_eff = min(k_end, original_seq_len)\n",
    "                q_start_eff = min(q_start, original_seq_len)\n",
    "                k_start_eff = min(k_start, original_seq_len)\n",
    "\n",
    "                if q_start_eff >= q_end_eff or k_start_eff >= k_end_eff:\n",
    "                    attention_sum = 0.0\n",
    "                    num_vals = 1\n",
    "                else:\n",
    "                    sub_matrix = token_attention_matrix[q_start_eff:q_end_eff, k_start_eff:k_end_eff]\n",
    "                    attention_sum = np.sum(sub_matrix)\n",
    "                    num_vals = sub_matrix.size if sub_matrix.size > 0 else 1\n",
    "\n",
    "                if group_aggregation_method == \"mean\":\n",
    "                    grouped_attention_matrix[i, j] = attention_sum / num_vals\n",
    "                elif group_aggregation_method == \"sum\":\n",
    "                    grouped_attention_matrix[i, j] = attention_sum\n",
    "                else:\n",
    "                    raise ValueError(\"group_aggregation_method must be 'sum' or 'mean'\")\n",
    "\n",
    "        plot_attention_matrix = grouped_attention_matrix\n",
    "        effective_tokens_x = group_names\n",
    "        effective_tokens_y = group_names\n",
    "\n",
    "    # --- Normalization choice ---\n",
    "    if normalization == \"row_percent\":\n",
    "        plot_attention_matrix = _row_percent(plot_attention_matrix, axis=1)\n",
    "        cbar_label = \"Row % (each row sums to 100)\"\n",
    "        vmin, vmax = 0, 100\n",
    "    elif normalization == \"softmax\":\n",
    "        plot_attention_matrix = _softmax(plot_attention_matrix, axis=1)\n",
    "        cbar_label = \"Probability (softmax over keys)\"\n",
    "        vmin, vmax = None, None\n",
    "    elif normalization == \"none\":\n",
    "        cbar_label = \"Attention Score\"\n",
    "        vmin, vmax = None, None\n",
    "    else:\n",
    "        raise ValueError(\"normalization must be 'row_percent', 'softmax', or 'none'\")\n",
    "\n",
    "    # Plot\n",
    "    # fig, ax = plt.subplots(figsize=figsize)\n",
    "    # im = ax.imshow(plot_attention_matrix, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "\n",
    "    # ax.set_xticks(np.arange(len(effective_tokens_x)))\n",
    "    # ax.set_yticks(np.arange(len(effective_tokens_y)))\n",
    "\n",
    "    # ax.set_xticklabels(effective_tokens_x, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # ax.set_yticklabels(effective_tokens_y)\n",
    "\n",
    "    # cbar = fig.colorbar(im, ax=ax)\n",
    "    # cbar.set_label(cbar_label)\n",
    "\n",
    "    # ax.set_xlabel(\"Key Tokens/Groups (Attended To)\")\n",
    "    # ax.set_ylabel(\"Query Tokens/Groups (Attending From)\")\n",
    "    # ax.set_title(f\"Attention - Layer {layer_idx}, Head {head_idx} (Batch {batch_idx}){title_suffix}\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"attention_vis/top_head/attention_visualization_{layer_idx}_{head_idx}.pdf\", dpi=300)\n",
    "    # plt.show()\n",
    "\n",
    "    return plot_attention_matrix\n",
    "\n",
    "def get_groups(tokens):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples, where each tuple is (start_idx, end_idx (exclusive), group_name).\n",
    "    \"\"\"\n",
    "    token_groups = []\n",
    "\n",
    "    # get bos group start and end token\n",
    "    bos_start = tokens.index(\"<bos>\")\n",
    "    bos_end = tokens.index(\"<start_of_turn>\")\n",
    "    # token_groups.append((bos_start, bos_end, \"BOS\"))\n",
    "\n",
    "    # get system prompt group start and end token\n",
    "    sys_prompt_start = tokens.index(\"<start_of_turn>\")\n",
    "    sys_prompt_end = tokens.index(\"\\n\\n\")+1\n",
    "    # token_groups.append((sys_prompt_start, sys_prompt_end, \"System Prompt\"))\n",
    "\n",
    "    # get user question group start and end token\n",
    "    user_q_start = tokens.index(\"\\n\\n\")+1\n",
    "    if \"?\" in tokens:\n",
    "        user_q_end = tokens.index(\"?\")+1\n",
    "    else:\n",
    "        user_q_end = tokens.index('\"?', user_q_start)\n",
    "    token_groups.append((user_q_start, user_q_end, \"Question\"))\n",
    "\n",
    "    # get user hypothesis group start and end token\n",
    "    if \"?\" in tokens:\n",
    "        user_hyp_start = tokens.index(\"?\")+1\n",
    "    else:\n",
    "        user_hyp_start = tokens.index('\"?', user_q_end)\n",
    "    user_hyp_end = tokens.index('▁but')\n",
    "    token_groups.append((user_hyp_start, user_hyp_end, \"User Hypothesis\"))\n",
    "\n",
    "    # get user doubt group start and end token\n",
    "    user_doubt_start = tokens.index('▁but')\n",
    "    user_doubt_end = tokens.index(\"<end_of_turn>\", user_doubt_start)\n",
    "    token_groups.append((user_doubt_start, user_doubt_end, \"User Doubt\"))\n",
    "\n",
    "\n",
    "    # get special turn separator\n",
    "    turn_start = tokens.index(\"<end_of_turn>\", user_doubt_start)\n",
    "    turn_end = tokens.index(\"<start_of_turn>\", user_doubt_start)\n",
    "    # token_groups.append((turn_start, turn_end, \"Turn Separator\"))\n",
    "\n",
    "    # get model answer group start and end token\n",
    "    model_ans_start = tokens.index(\"\\n\", turn_end)+1\n",
    "    model_ans_end = tokens.index(\"<end_of_turn>\", model_ans_start)\n",
    "    token_groups.append((model_ans_start, model_ans_end, \"Model Answer\"))\n",
    "\n",
    "    # get end token group start and end token\n",
    "    end_start = tokens.index(\"<end_of_turn>\", model_ans_start)\n",
    "    # token_groups.append((end_start, len(tokens), \"End\"))\n",
    "\n",
    "    return token_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Use doubt (0, 5)', 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "nums = [random.randint(0, len(tokenized_data)) for _ in range(8)]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "ans_to_doubt_scores = []\n",
    "ans_to_hypothesis_scores = []\n",
    "\n",
    "for key, value in tqdm(sorted_items[:64]):\n",
    "    ans_to_doubt = []\n",
    "    ans_to_hypothesis = []\n",
    "    for num in nums:\n",
    "        layer, head = key.split(\"_\")\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=tokenized_data[num].to('cuda').unsqueeze(0), output_attentions=True)\n",
    "        tokens = tokenizer.tokenizer.convert_ids_to_tokens(tokenized_data[num].tolist()) # For the first (and only) batch item\n",
    "        token_groups_definition = get_groups(tokens)\n",
    "        matrix = visualize_attention(\n",
    "            all_layer_attentions=output.attentions,\n",
    "            layer_idx=int(layer),\n",
    "            head_idx=int(head),\n",
    "            tokens=tokens,\n",
    "            batch_idx=0,\n",
    "            token_groups=token_groups_definition,\n",
    "            group_aggregation_method=\"sum\",\n",
    "            figsize=(8,6),\n",
    "            normalization='row_percent'\n",
    "        )\n",
    "        ans_to_doubt.append(matrix[-1, -2])\n",
    "        ans_to_hypothesis.append(matrix[-1, -3])\n",
    "    ans_to_doubt_scores.append(np.mean(ans_to_doubt))\n",
    "    ans_to_hypothesis_scores.append(np.mean(ans_to_hypothesis))\n",
    "\n",
    "sum_scores = np.array(ans_to_doubt_scores) + np.array(ans_to_hypothesis_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_accuracies = [x[1] for x in sorted_items]\n",
    "import numpy as np\n",
    "\n",
    "r = np.corrcoef(probe_accuracies[:64], sum_scores)[0, 1]\n",
    "print(r)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(probe_accuracies, sum_scores, color='blue', alpha=0.7, edgecolors='k')  # points\n",
    "plt.xlabel(\"Probe Accurcies\")\n",
    "plt.ylabel(\"Sum of Attention Scores (Answer to Doubt + Answer to Hypothesis)\")\n",
    "plt.title(\"Scatter Plot of A vs. B\")\n",
    "\n",
    "# Optional: add a trendline\n",
    "import numpy as np\n",
    "m, c = np.polyfit(probe_accuracies, sum_scores, 1)  # slope & intercept\n",
    "plt.plot(probe_accuracies, np.array(probe_accuracies)*m + c, color='red', linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_head_hypothesis_score = np.mean(hypothesis_scores)\n",
    "top_head_doubt_score = np.mean(doubt_scores)\n",
    "top_other_score = np.mean(other_scores)\n",
    "top_head_hypothesis_score, top_head_doubt_score, top_other_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_head_hypothesis_score = np.mean(hypothesis_scores)\n",
    "random_head_doubt_score = np.mean(doubt_scores)\n",
    "random_other_score = np.mean(other_scores)\n",
    "random_head_hypothesis_score, random_head_doubt_score, random_other_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Input Data ---\n",
    "\n",
    "top_heads_scores = {\n",
    "    'User Hypothesis': 2.6334276,\n",
    "    'User Doubt':0.33000344,\n",
    "    'Other Prompt Sections': 8.967411254827311\n",
    "}\n",
    "\n",
    "\n",
    "# Group 2: Random 64 Heads\n",
    "random_heads_scores = {\n",
    "    'User Hypothesis': 1.1552463,\n",
    "    'User Doubt': 0.19743735,\n",
    "    'Other Prompt Sections': 10.469962649672667\n",
    "}\n",
    "\n",
    "# --- Data Preparation for Plotting ---\n",
    "group_labels = ['Top related Heads', 'Least related Heads']\n",
    "category_labels = list(top_heads_scores.keys()) # Ensure consistent order\n",
    "\n",
    "# Calculate total scores for each group (across these categories)\n",
    "total_top = sum(top_heads_scores.values())\n",
    "total_random = sum(random_heads_scores.values())\n",
    "\n",
    "# Calculate percentages for each category within each group\n",
    "top_percentages = np.array([top_heads_scores[cat] / total_top * 100 for cat in category_labels])\n",
    "random_percentages = np.array([random_heads_scores[cat] / total_random * 100 for cat in category_labels])\n",
    "\n",
    "# Data for plotting (rows are categories, columns are groups)\n",
    "data_percentages = np.array([\n",
    "    [top_percentages[0], random_percentages[0]],  # Hypothesis\n",
    "    [top_percentages[1], random_percentages[1]],  # Doubt\n",
    "    [top_percentages[2], random_percentages[2]]   # Other\n",
    "])\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(9, 5.5))\n",
    "\n",
    "bar_width = 0.5\n",
    "\n",
    "# --- MODIFICATION: Shift bars to the right ---\n",
    "# Define a gap between the y-axis and the left edge of the first bar\n",
    "gap_from_yaxis = 0.3  # Adjust this value as needed for desired spacing\n",
    "# Calculate the center of the first bar\n",
    "first_bar_center_x = gap_from_yaxis + bar_width / 2\n",
    "# Create new x_positions, shifted by first_bar_center_x\n",
    "# The original x_positions started at 0 (e.g., [0, 1])\n",
    "# The new ones will start at first_bar_center_x (e.g., [0.35, 1.35] if gap=0.1, width=0.5)\n",
    "x_positions = np.arange(len(group_labels)) + first_bar_center_x\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "colors = ['#FFA07A', '#A2D2A2', '#ADD8E6'] # User Hypothesis (Salmon), User Doubt (Pastel Green), Other (Light Blue)\n",
    "\n",
    "# Plot stacked bars\n",
    "bottom_values = np.zeros(len(group_labels))\n",
    "\n",
    "for i, category in enumerate(category_labels):\n",
    "    bars = ax.bar(\n",
    "        x_positions, # Use new, shifted x_positions\n",
    "        data_percentages[i],\n",
    "        bar_width,\n",
    "        bottom=bottom_values,\n",
    "        label=category,\n",
    "        color=colors[i],\n",
    "        edgecolor='white'\n",
    "    )\n",
    "    for bar_idx, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        if height > 2:\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2.,\n",
    "                bottom_values[bar_idx] + height / 2.,\n",
    "                f'{height:.1f}%',\n",
    "                ha='center',\n",
    "                va='center',\n",
    "                color='black', # Changed from 'white' to 'black' for better readability on light bars\n",
    "                fontsize=9,\n",
    "                # fontweight='bold'\n",
    "            )\n",
    "    bottom_values += data_percentages[i]\n",
    "\n",
    "\n",
    "# --- Aesthetics and Labels ---\n",
    "ax.set_ylabel('Aggregated Attention (%)', fontsize=11)\n",
    "ax.set_title('Attention to Prompt Segments', fontsize=14, pad=15)\n",
    "ax.set_xticks(x_positions) # Use new, shifted x_positions for ticks\n",
    "ax.set_xticklabels(group_labels, rotation=0, ha='center', fontsize=10)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# --- MODIFICATION: Adjust x-axis limits ---\n",
    "# Set left x-limit to 0 so the y-axis is the leftmost boundary\n",
    "xlim_left = 0\n",
    "# Add some padding to the right of the last bar\n",
    "padding_right_of_bars = gap_from_yaxis + 0.1 # Make it symmetrical or slightly larger than left gap\n",
    "xlim_right = x_positions[-1] + bar_width / 2 + padding_right_of_bars\n",
    "ax.set_xlim(xlim_left, xlim_right)\n",
    "# --- END MODIFICATION ---\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "legend = ax.legend(title=\"Prompt Segment\", loc='upper left', bbox_to_anchor=(1.02, 1), frameon=False, fontsize=10)\n",
    "plt.setp(legend.get_title(),fontsize='11')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.82, 1])\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.6, linewidth=0.5)\n",
    "\n",
    "plt.savefig('attention_comparison_spaced.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalized Data for Verification:\")\n",
    "print(f\"Top Heads Percentages: {dict(zip(category_labels, top_percentages))}\")\n",
    "print(f\"Random Heads Percentages: {dict(zip(category_labels, random_percentages))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycophancy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
