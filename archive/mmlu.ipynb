{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_model\n",
    "\n",
    "model_id = 'gemma-3'\n",
    "model, processor = load_model(model_id)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_dict = load_dataset(\"cais/mmlu\", \"all\")\n",
    "\n",
    "# Let's say you want to sample from the 'test' split\n",
    "split_name = 'test'\n",
    "if split_name not in dataset_dict:\n",
    "    print(f\"Split '{split_name}' not found. Please choose from: {list(dataset_dict.keys())}\")\n",
    "    exit()\n",
    "\n",
    "ds_split = dataset_dict[split_name]\n",
    "print(f\"\\nSelected split '{split_name}' with {len(ds_split)} examples.\")\n",
    "\n",
    "# 3. Define the number of random samples you want\n",
    "n = 500  # For example, 5 random samples\n",
    "\n",
    "# 4. Ensure n is not larger than the dataset size\n",
    "if n > len(ds_split):\n",
    "    print(f\"Warning: n ({n}) is larger than the dataset size ({len(ds_split)}).\")\n",
    "    print(f\"Sampling all {len(ds_split)} available items instead.\")\n",
    "    n_samples_to_take = len(ds_split)\n",
    "else:\n",
    "    n_samples_to_take = n\n",
    "\n",
    "# 5. Shuffle the chosen split and select the first n items\n",
    "random_sample_dataset = ds_split.shuffle(seed=42).select(range(n_samples_to_take))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"\n",
    "def generate_and_decode_new_tokens(prompt, model, processor, model_id, max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    Generate a response to a prompt and decode only the new tokens.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt text\n",
    "        model: The language model to use for generation\n",
    "        processor: The tokenizer/processor for encoding/decoding\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        str: The decoded text from only the newly generated tokens\n",
    "    \"\"\"\n",
    "    if model_id == 'gemma-3':\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_prompt}]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt + \" Give me your best guess and answer as concisely as possible.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt + \" Give me your best guess and answer as concisely as possible.\"\n",
    "            }\n",
    "        ]    \n",
    "    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        \n",
    "    if model_id == 'gemma-3':\n",
    "        inputs = processor(text=inputs, return_tensors=\"pt\").to('cuda')\n",
    "    else:\n",
    "        inputs = processor(inputs, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        if 'intervenable' in str(type(model)).lower():\n",
    "            _, generation = model.generate(inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        else:\n",
    "            generation = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "        new_tokens = generation[0][input_len:]\n",
    "\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    res_1 = processor.decode(new_tokens, skip_special_tokens=True)\n",
    "    return res_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_test = random_sample_dataset['question']\n",
    "answers_idxs = random_sample_dataset['answer']\n",
    "choices_test = random_sample_dataset['choices']\n",
    "answers_test = [x[answers_idxs[i]] for i, x in enumerate(choices_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "predictions = []\n",
    "for question in tqdm(questions_test):\n",
    "    res = generate_and_decode_new_tokens(question, model, processor, model_id)\n",
    "    predictions.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import to_request, create_anthropic_batch_job\n",
    "\n",
    "requests = [to_request(f'mmlu_{model_id}_base-{i}', q, a, p) \n",
    "                for i, (q, a, p) in enumerate(zip(questions_test, answers_test, predictions))]\n",
    "\n",
    "import json\n",
    "with open(f\"mmlu-{model_id}_base.jsonl\", 'w') as f:\n",
    "    for item in requests:\n",
    "        json_line = json.dumps(item)\n",
    "        f.write(json_line + '\\n')\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"\")\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(f\"mmlu-{model_id}_base.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "batch_input_file_id = batch_input_file.id\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": f\"mmlu-{model_id}_base\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sycophancy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
